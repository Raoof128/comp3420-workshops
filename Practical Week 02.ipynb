{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to github classroom\n",
    "\n",
    "Some of the assignments in this unit will be managed via GitHub classroom. Please follow these steps to connect:\n",
    "\n",
    "1. Follow this invitation link and accept the invitation: https://classroom.github.com/a/JzzNHoZ_2. The link may ask you to sign in to GitHub (if you haven't signed in earlier). If you don't have a GitHub account, you will need to register.\n",
    "3. Once you have logged in with GitHub, you may need to select your email address to associate your GitHub account with your email address (if you haven't done it in a previous COMP3420 activity). If you can't find your email address, please skip this step and contact xiaohan.yu@mq.edu.au so that he can do the association manually.\n",
    "4. Wait a minute or two, and refresh the browser until it indicates that your assignment repository has been created. Your repository is private to you, and you have administration privileges. Only you and the lecture will have access to it. The repository will be listed under the list of repositories belonging to this offering of COMP3420: https://github.com/orgs/2025S2COMP64203420/repositories5. Your assignment repository will include starter code that you can use for the exercises below. Clone your repository into a folder in your computer.\n",
    "\n",
    "This practical has two kinds of exercises:\n",
    "\n",
    "1. **Implement functions and upload the implementation to github classroom**. The exercises will have associated automated tests. To run these tests, please commit your changes and push the changes to your repository. This will initiate the automated tests, and you will receive the test results. There are no marks associated with these tests, but they will help you get used to the environment that you will use for the assignments.\n",
    "2. **Analyse the data, train and evaluate image classifiers.** These exercises do not have automated tests but they will help you practice with the kinds of tasks that you will need to do in the assignments.\n",
    "\n",
    "\n",
    "# 2. Classify digits using the MNIST dataset\n",
    "\n",
    "For the exercises of this section, we will use the MNIST dataset. This dataset is already available in TorchVision, and we have used it in the lectures.\n",
    "\n",
    "## 2.1 Count labels\n",
    "\n",
    "Write a function `summary_mnist` that returns the counts of labels in the training set and the test set. The function receives the following:\n",
    "  - `train_labels`: the list of labels from the training set.\n",
    "  - `test_labels`: the list of labels from the test dataset.\n",
    "  - `labels_list`: the list of unique labels from the dataset.\n",
    "  The output must be a Python dictionary with the following two items:\n",
    "  -  `\"train_labels_counts\"`: The counts of occurrences in `train_labels` for each label listed in `labels_list`.\n",
    "  -  `\"test_labels_counts\"`: The counts of occurrences in `test_labels` for each label listed in `labels_list`.\n",
    "\n",
    "An example of usage of this function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T02:33:14.855140Z",
     "iopub.status.busy": "2025-08-13T02:33:14.854950Z",
     "iopub.status.idle": "2025-08-13T02:33:17.334493Z",
     "shell.execute_reply": "2025-08-13T02:33:17.333811Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"The device is\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3fab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T02:33:17.399349Z",
     "iopub.status.busy": "2025-08-13T02:33:17.398956Z",
     "iopub.status.idle": "2025-08-13T02:33:17.696953Z",
     "shell.execute_reply": "2025-08-13T02:33:17.696233Z"
    }
   },
   "outputs": [],
   "source": [
    "data_folder = \"MNIST\"\n",
    "train_mnist = datasets.MNIST(data_folder, download=True, train=True)\n",
    "test_mnist = datasets.MNIST(data_folder, download=True, train=False)\n",
    "\n",
    "train_images = train_mnist.data\n",
    "train_targets = train_mnist.targets\n",
    "test_images = test_mnist.data\n",
    "test_targets = test_mnist.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T02:33:17.699781Z",
     "iopub.status.busy": "2025-08-13T02:33:17.699550Z",
     "iopub.status.idle": "2025-08-13T02:33:18.074800Z",
     "shell.execute_reply": "2025-08-13T02:33:18.074218Z"
    }
   },
   "outputs": [],
   "source": [
    "import week2\n",
    "labels_counts = week2.summary_mnist(train_targets.tolist(), test_targets.tolist(), range(10))\n",
    "labels_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that label 0 has a count of 5923 in the training data and 980 in the test data, and so on with labels 1, 2, ... 9. With this information you can, for example, do these plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T02:33:18.077058Z",
     "iopub.status.busy": "2025-08-13T02:33:18.076782Z",
     "iopub.status.idle": "2025-08-13T02:33:18.305371Z",
     "shell.execute_reply": "2025-08-13T02:33:18.304758Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.bar(range(10), labels_counts['train_labels_counts'])\n",
    "plt.title(\"Counts of labels in the training data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T02:33:18.307755Z",
     "iopub.status.busy": "2025-08-13T02:33:18.307533Z",
     "iopub.status.idle": "2025-08-13T02:33:18.396601Z",
     "shell.execute_reply": "2025-08-13T02:33:18.395975Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.bar(range(10), labels_counts['test_labels_counts'])\n",
    "plt.title(\"Counts of labels in the test data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "1. Are the data balanced? (in other words, do all labels have a similar number of counts?)  \n",
    "   **Yes.** Each digit occurs roughly the same number of times (about 5–7k in train and 1k in test).\n",
    "\n",
    "2. Do train and test data have similar label distributions?  \n",
    "   **Yes.** The distributions of digits are very similar between training and test sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Classify digits\n",
    "\n",
    "Write a function `build_mnist_model` that returns a pytorch model to classify the MNIST data. The resulting model must contain a hidden layer with a ReLU activation, followed by a dropout layer (optional), and the final classification layer. As in the notebook from the lectures, assume that the images have been flattened to a vector of size 28 * 28 before they are processed by the model. Make sure that the last layer has the correct size.\n",
    "\n",
    "*Hint: to add a dropout layer, use `Dropout`. Read this from PyTorch's documentation: https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html*\n",
    "\n",
    "The function has the following parameters:\n",
    "- `hidden_size`: size of the hidden layer.\n",
    "- `droput_rate`: Dropout rate of the dropout layer (which is placed after the hidden layer). If the dropout rate is zero, there should be no dropout layer.\n",
    "\n",
    "An example of usage of this function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T02:33:18.398714Z",
     "iopub.status.busy": "2025-08-13T02:33:18.398500Z",
     "iopub.status.idle": "2025-08-13T02:33:18.402848Z",
     "shell.execute_reply": "2025-08-13T02:33:18.402354Z"
    }
   },
   "outputs": [],
   "source": [
    "import week2\n",
    "mnist_model = week2.build_mnist_model(64, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of this function, train the classifier using different hyperparameters (in this exercise, the hyperparameters are the hidden layer size and dropout rate). Answer the following questions:\n",
    "\n",
    "1. What are the optimal hyperparameters?\n",
    "2. Does the model overfit? How can you determine whether it is overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T02:33:19.364171Z",
     "iopub.status.busy": "2025-08-13T02:33:19.363635Z",
     "iopub.status.idle": "2025-08-13T02:33:55.350738Z",
     "shell.execute_reply": "2025-08-13T02:33:55.350010Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import week2\n",
    "from torch import nn\n",
    "\n",
    "train_dataset = datasets.MNIST(data_folder, train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(data_folder, train=False, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_and_eval(hidden_size, dropout, epochs=1):\n",
    "    model = week2.build_mnist_model(hidden_size, dropout).to(device)\n",
    "    optim = torch.optim.Adam(model.parameters())\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            data = data.view(data.size(0), -1).to(device)\n",
    "            target = target.to(device)\n",
    "            optim.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = loss_fn(out, target)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "    def accuracy(loader):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in loader:\n",
    "                data = data.view(data.size(0), -1).to(device)\n",
    "                target = target.to(device)\n",
    "                pred = model(data).argmax(dim=1)\n",
    "                correct += pred.eq(target).sum().item()\n",
    "        return correct / len(loader.dataset)\n",
    "    return model, accuracy(train_loader), accuracy(test_loader)\n",
    "\n",
    "best_model = None\n",
    "best_acc = 0\n",
    "for hidden, drop in [(64,0), (64,0.5), (128,0.5)]:\n",
    "    model, train_acc, test_acc = train_and_eval(hidden, drop)\n",
    "    print(f\"hidden={hidden}, drop={drop}: train_acc={train_acc:.3f}, test_acc={test_acc:.3f}\")\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        best_model = model\n",
    "model = best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing configuration above was **hidden_size=128** with **dropout_rate=0.5**, giving about 94% accuracy after one epoch.\n",
    "Training and test accuracies were very close for all runs, suggesting little overfitting. Overfitting would appear as a much higher training accuracy than test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Using the webcam\n",
    "\n",
    "The following code illustrates how you can classify the first 5 images of the test data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T02:33:55.353077Z",
     "iopub.status.busy": "2025-08-13T02:33:55.352856Z",
     "iopub.status.idle": "2025-08-13T02:33:55.439521Z",
     "shell.execute_reply": "2025-08-13T02:33:55.438839Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,1+i)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(test_images[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T02:33:55.441634Z",
     "iopub.status.busy": "2025-08-13T02:33:55.441440Z",
     "iopub.status.idle": "2025-08-13T02:33:55.453132Z",
     "shell.execute_reply": "2025-08-13T02:33:55.452433Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    image = test_images[i].to(device)\n",
    "    image = image.float()/255    # Scale the data\n",
    "    image = image.view(28*28) # Flatten the image\n",
    "    print(\"The target label is\", test_targets[i].item())\n",
    "    print(\"The predicted label is\", model(image).argmax().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on that, write code that uses the webcam to classify handwritten items. You can, for example, follow these steps:\n",
    "1. Write or re-use the lecture code that captures an image from the webcam. Display the image, to make sure that the image captured is the intended one.\n",
    "2. Write code that applies the best model that you have generated in exercise 2 to the image captured and prints the predicted label.\n",
    "\n",
    "The above steps are a suggestion, you may do it differently. Now, write numbers using a thick pen on a white paper, and use the webcam and your code to read them. How did the classification system perform? Does the size and position of the written number in the image affect the classification result?\n",
    "\n",
    "- *Hint 1: You will need to convert the image captured by the webcam to grayscale. Using OpenCV, you can do this: `cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)`*\n",
    "- *Hint 2: If you need to zoom into the image, you can do something like `cropped = image[100:300, 100:300]`*\n",
    "- *Hint 3: You will need to resize the captured image. Use, for example, `cv2.resize(image, (28, 28))`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T02:33:55.455503Z",
     "iopub.status.busy": "2025-08-13T02:33:55.455282Z",
     "iopub.status.idle": "2025-08-13T02:33:55.459694Z",
     "shell.execute_reply": "2025-08-13T02:33:55.459126Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here. Feel free to add more code and text cells if necessary.\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def classify_from_webcam():\n",
    "    \"\"\"Capture image from webcam and classify handwritten digit\"\"\"\n",
    "    \n",
    "    # Step 1: Capture image from webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Failed to capture image from webcam\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Apply the model to the captured image\n",
    "    \n",
    "    # Convert to grayscale (Hint 1)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Crop center region (Hint 2)\n",
    "    h, w = gray.shape\n",
    "    size = min(h, w) // 2\n",
    "    y1, y2 = h//2 - size//2, h//2 + size//2\n",
    "    x1, x2 = w//2 - size//2, w//2 + size//2\n",
    "    cropped = gray[y1:y2, x1:x2]\n",
    "    \n",
    "    # Resize to 28x28 (Hint 3)\n",
    "    resized = cv2.resize(cropped, (28, 28))\n",
    "    \n",
    "    # Invert colors for MNIST format\n",
    "    inverted = 255 - resized\n",
    "    \n",
    "    # Normalize and convert to tensor\n",
    "    image_tensor = torch.from_numpy(inverted.astype(np.float32) / 255.0)\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    image_tensor = image_tensor.view(28*28)  # Flatten like in the example\n",
    "    \n",
    "    # Predict using the model\n",
    "    predicted_label = model(image_tensor).argmax().item()\n",
    "    \n",
    "    # Display the captured image\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Captured Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(gray, cmap='gray')\n",
    "    plt.title('Grayscale')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(cropped, cmap='gray')\n",
    "    plt.title('Cropped')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.imshow(resized, cmap='gray')\n",
    "    plt.title(f'28x28\\nPredicted: {predicted_label}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the predicted label\n",
    "    print(\"The predicted label is\", predicted_label)\n",
    "\n",
    "# Test with webcam\n",
    "classify_from_webcam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Webcam Digit Classification\n",
    "\n",
    "The webcam classification system performance depends on several factors:\n",
    "\n",
    "### Key Implementation Features:\n",
    "1. **Image Preprocessing Pipeline:**\n",
    "   - Converts webcam RGB image to grayscale\n",
    "   - Crops to center region to focus on the digit\n",
    "   - Resizes to 28×28 pixels (MNIST format)\n",
    "   - Inverts colors (black text on white background → white text on black background)\n",
    "   - Normalizes pixel values to [0,1] range\n",
    "\n",
    "2. **Real-time Capture:**\n",
    "   - Uses OpenCV to access webcam\n",
    "   - Interactive capture with spacebar\n",
    "   - Visual feedback showing original, grayscale, and processed images\n",
    "\n",
    "### Expected Performance Issues:\n",
    "\n",
    "**Size and Position Effects:**\n",
    "- **Size**: Numbers that are too small or too large relative to the frame will be poorly resized to 28×28\n",
    "- **Position**: Off-center digits may be cropped incorrectly, affecting recognition\n",
    "- **Solution**: The code crops to center region to mitigate positioning issues\n",
    "\n",
    "**Other Factors Affecting Classification:**\n",
    "- **Lighting conditions**: Poor lighting can affect contrast\n",
    "- **Writing style**: MNIST digits are quite standardized; very different handwriting styles may not work well\n",
    "- **Paper quality**: Wrinkled or non-white backgrounds can interfere\n",
    "- **Camera quality**: Low resolution cameras will provide poor input\n",
    "\n",
    "### Recommendations for Better Results:\n",
    "1. Write digits large and centered in the camera view\n",
    "2. Use thick, dark pen on clean white paper\n",
    "3. Ensure good, even lighting\n",
    "4. Hold paper steady and flat\n",
    "5. Try different distances from camera to find optimal size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Post-practical questionnaire\n",
    "\n",
    "Please complete the week 2 post-practical questionnaire on iLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
